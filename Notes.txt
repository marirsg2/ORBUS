
NOTICE: that after the 12th or so round (intersection point), OBUS does BETTER !! HIGH noise, obus focusses on value
more than noise. I imagine it will oscillate, so can't say for certain.


----------------------

0) NEXT: SCORE based on the ordering of the top scorers. i.e. do one of these
1) For top 5, plans If the order is inverted or wrong increase error. The top plan error is the highest weight 1.0
second plan is weighted half as much , and so forth.
2) Score the plans based on how far off their position is to their true position. NO this penalizes small differences arbitarirly
 SO INSTEAD: COMPUTE THE DIFFERENCE IN VALUE BETWEEN the true plan for that position, and your chosen plan.
 Then WEIGHT the difference based on the position, or value.


SEE survey paper on RANKING preferences research !!

1) UCB/thompson approach WORKS REALLY REALLY WELL !!.

    tHE REAL THOMPSON sampling approach actually samples from the distribution to determine the gain value. yours is more
    of UCB. Where you assume gaussian and 2 std dev = 95% confidence. It is only related DO NOT EQUATE.
    I THINK IF YOU HAVE A SCHEDULE OF REDUCING THE NUM OF STD DEV TO USE, then it might improve accuracy at the very top,
    ? but this may not be a good thing ? you want SOME knowledge of all regions. BUT the latter comes auto as the main
    focus IS variance, not gain. NEED some testing. Maybe future work. Along with some theoretical analysis like regret

     MAYBE an interesting metric is amount of reward accrued !! at each point, you get to choose the top 5 samples.
     The value in the top 5 (order does not matter within top 5) that you got correct, is the money made by the ad-choosing
     alg that used OBUS. MAYBE ORDER SHOULD MATTER. So give weight 1 to the top position, weight 0.5 the second and so forth
     reducing weight by factor of 1/2

     Compare with the simple expected gain version. That seems to do well earlier a little more, BUT i dont think it will be
     as accurate in the higher regions. The UCB version will. AND FOR THIS, the test about ranking the top 5 (or 10) is better.
     The ranking is computed at each round. I imagine for variance it will fluctuate more.

     BUT note that in the overall error, it is faster until about the point where the error and noise match,
     then it spikes and then drops back down. I think that spike has to do with reconciling the weights of high feature
     weights with the lower weight features that it is now working on. This spike is NOT there in the high extreme regions [0.1,0.9]
     and only in the overall error.

2) The argument for sampling based on output value rather than features of high value and which ones cooccur, is because we ALREADY consider
the FREQUENCY of the features, and their cooccurrence in the feature frequency term !!
3) Multi-arm bandits for online ad selection is worse !? You are limited to the arms or EVEN factored arm definitions that
are available to you. Ours is a more natural model for the online ad scenario. There is no ONE best arm or arms.
Ads may change in nature, but features would persist, and more useful. Multiple models maybe trained at different times,
and we sample using the model that has been the most accurate in the recent past (we present top 5, and the model that
was right more often, is chosen).

3) ALL WE ARE DOING is changing the order of weights to improve accuracy on. We do NOT get any miraculous gains in overall speed.
just in terms of what we sample first.
THE OPPOSITE of what we are doing is if we use 1/gain

!! v2 works with gaussian !! ?? more tail, more effect ? test with gumbel

BECAUSE of the interconnected nature of the data.
TWO GROUPS you may see a difference
Then add an intersection variable, and vary the frequency of it. Bottle neck of the info flow.
Then have 3 groups of variables, with


v1) F_beta should have beta between 1 and 2 ? or problem specific. If uniform distribution, then lower ?? if gaussian or gumbel
then weight it higher. TEST theory that gain based sampling provides support for high weight features more than low weight features.

v2) var + var^g should be compared with F_beta
USE TWO GROUPS. Use gaussian

----------------------

@ Groups of smaller features that add up make us better ? shows the benefit of non-blind variance search.

@ try the other gain function

@ larger features

----------------------
========FUTURE WORK
1) ERROR/value loss can be used in NN too !! selective training for accuracy. Ignore features that occur in weaker regions.
        FOCUS ON FEATURES THAT CONTRIBUTE MORE !! NN-regression.
2) Add in F_beta score for sampling the data points.

-----------------