



rANDOM does so much better when all features are known and relevant ? it's early discovery and
subsequent gains.

WHY IS THE FIRST ROUND ERROR DIFFERENT ? we should be sampling THE EXACT SAME PLANS ?? test this
DEBUG SLOW. as of now 20 features, 4 per plan. 10k plans easily

The update is exactly the SAME !!?? debug !!


?? IT IS DOING THE OPPOSITE. overall the error is lower, but in the extreme regions, not so ?

It was ZERO sigma ?? now lets try with 2 sigma. with zero sigma, the average was barely better
and extremes were about equal ?? how ?

Retry sigma + sigma^mu
Also (1+mu)^(sigma)
MAKE SURE to test with RANDOM as well. Easy to implement, set base score to random.
NOTE that random works really well, it discovers features and if your variance estimate is bad. Random is
better.

Reduce plans per round to 3

Add back probability term TO JUST your method. Does it do better? Then ONLY  have probability terms. Obviously that would do better.
But if both have probability, does OBUS succeed again ? It could be that the probability term (badly calculated was hurting)

1 Round per plan ? sharper difference than with 5 per round.

FUCK ! you currently had probability and discovery terms on. Mistake. That is for later
NOW only discovery. also SET THE plan generated to be uniform prob

MAYBE set all terms as relevant. and fewer feature and plans .

COMPARE WITH RANDOM SAMPLING. important

print TOP K ORDER.

Redo with std dev noise = 0.3
Do uniformXgaussian with 0.4 features
The bump up maybe due to the rarity  of strong or weak features, , and so it accounts for less ?
WHAT happens with gumbel distribution vs gaussian. more big features, and 0.4 and thompson sampling



TRY: (1+norm_sigma)^(1+mu)

Expect gaussian X gaussian with MORE features , will show a larger benefit at the start.
NEXT: gxg with feature prob  =0.2


Do with 3 groups. High , med, and low feature weights. Uniform probability for choosing each group and feat distr for each,
and uniform (NOT GAUSSIAN) feature weights in each group, with overlapping regions like [10,-3],[3,-3],[3,-10]

feature probability is IMPORTANT. I wonder if you include it , THE PROPER computation, not the hacky one you did
earlier, then gausXgaus will be much better too ?? It would be another addendum to the correction in the exponent ?
ideally the form would be (1+sigma)^(1+norm_mu+norm_prob). That would be nice, but expect otherwise , isnt that always
what happens.

LATER: matrix of cooccurrence POTENTIALS (easier, and then convert to probabilities). Note the self potential, is
auto occurrence, irrespective of others, same feature will not occur twice.
MORE ELEGANT AND BETTER, AND SO DO THIS: Build a bayesian tree. The roots may not be feature variables, rather
conditioning variables. MRF and Bayes graph are different paradigms and information flow maybe different.


----------------------

0) NEXT: SCORE based on the ordering of the top scorers. i.e. do one of these
1) For top 5, plans If the order is inverted or wrong increase error. The top plan error is the highest weight 1.0
second plan is weighted half as much , and so forth.
2) Score the plans based on how far off their position is to their true position. NO this penalizes small differences arbitarirly
 SO INSTEAD: COMPUTE THE DIFFERENCE IN VALUE BETWEEN the true plan for that position, and your chosen plan.
 Then WEIGHT the difference based on the position, or value.


SEE survey paper on RANKING preferences research !!

1) UCB/thompson approach WORKS REALLY REALLY WELL !!.

    tHE REAL THOMPSON sampling approach actually samples from the distribution to determine the gain value. yours is more
    of UCB. Where you assume gaussian and 2 std dev = 95% confidence. It is only related DO NOT EQUATE.
    I THINK IF YOU HAVE A SCHEDULE OF REDUCING THE NUM OF STD DEV TO USE, then it might improve accuracy at the very top,
    ? but this may not be a good thing ? you want SOME knowledge of all regions. BUT the latter comes auto as the main
    focus IS variance, not gain. NEED some testing. Maybe future work. Along with some theoretical analysis like regret

     MAYBE an interesting metric is amount of reward accrued !! at each point, you get to choose the top 5 samples.
     The value in the top 5 (order does not matter within top 5) that you got correct, is the money made by the ad-choosing
     alg that used OBUS. MAYBE ORDER SHOULD MATTER. So give weight 1 to the top position, weight 0.5 the second and so forth
     reducing weight by factor of 1/2

     Compare with the simple expected gain version. That seems to do well earlier a little more, BUT i dont think it will be
     as accurate in the higher regions. The UCB version will. AND FOR THIS, the test about ranking the top 5 (or 10) is better.
     The ranking is computed at each round. I imagine for variance it will fluctuate more.

     BUT note that in the overall error, it is faster until about the point where the error and noise match,
     then it spikes and then drops back down. I think that spike has to do with reconciling the weights of high feature
     weights with the lower weight features that it is now working on. This spike is NOT there in the high extreme regions [0.1,0.9]
     and only in the overall error.

2) The argument for sampling based on output value rather than features of high value and which ones cooccur, is because we ALREADY consider
the FREQUENCY of the features, and their cooccurrence in the feature frequency term !!
3) Multi-arm bandits for online ad selection is worse !? You are limited to the arms or EVEN factored arm definitions that
are available to you. Ours is a more natural model for the online ad scenario. There is no ONE best arm or arms.
Ads may change in nature, but features would persist, and more useful. Multiple models maybe trained at different times,
and we sample using the model that has been the most accurate in the recent past (we present top 5, and the model that
was right more often, is chosen).

3) ALL WE ARE DOING is changing the order of weights to improve accuracy on. We do NOT get any miraculous gains in overall speed.
just in terms of what we sample first.
THE OPPOSITE of what we are doing is if we use 1/gain

!! v2 works with gaussian !! ?? more tail, more effect ? test with gumbel

BECAUSE of the interconnected nature of the data.
TWO GROUPS you may see a difference
Then add an intersection variable, and vary the frequency of it. Bottle neck of the info flow.
Then have 3 groups of variables, with


v1) F_beta should have beta between 1 and 2 ? or problem specific. If uniform distribution, then lower ?? if gaussian or gumbel
then weight it higher. TEST theory that gain based sampling provides support for high weight features more than low weight features.

v2) var + var^g should be compared with F_beta
USE TWO GROUPS. Use gaussian

----------------------

@ Groups of smaller features that add up make us better ? shows the benefit of non-blind variance search.

@ try the other gain function

@ larger features

----------------------
========FUTURE WORK
1) ERROR/value loss can be used in NN too !! selective training for accuracy. Ignore features that occur in weaker regions.
        FOCUS ON FEATURES THAT CONTRIBUTE MORE !! NN-regression.
2) Add in F_beta score for sampling the data points.

-----------------